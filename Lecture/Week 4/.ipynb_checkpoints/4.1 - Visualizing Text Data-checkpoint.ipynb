{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualizing Text Data\n",
    "\n",
    "### What Makes Visualizing Text Data Different?\n",
    "\n",
    "1. **High Dimensionality**: Text data often comes in a high-dimensional format. A single document or piece of text can contain hundreds or thousands of unique words (features), making it challenging to visualize directly.\n",
    "\n",
    "2. **Unstructured Data**: Unlike numerical or categorical data, text data is unstructured, requiring more sophisticated pre-processing techniques to transform it into a useful format for analysis.\n",
    "\n",
    "3. **Sparsity**: Text data matrices are typically sparse. Most elements are zero, which poses challenges for visualization.\n",
    "\n",
    "4. **Lack of Inherent Order**: Words in a text don't have an inherent order of importance unless processed to determine such an order.\n",
    "\n",
    "5. **Semantics**: The meaning carried by words and their context often needs to be captured for effective visualization, requiring methods like sentiment analysis, topic modeling, or word embeddings.\n",
    "\n",
    "6. **Multilevel Features**: Texts can be described by both the words they contain and metadata like author, time of creation, or source, which may also be essential for analysis and visualization.\n",
    "\n",
    "However, much of the data we encounter in the world is text data, so it's helpful to learn some ways to visualize relationships in text.  We'll discuss three options in this lesson, but know that there are many many more ways to look at text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Working with text data falls under the heading of Natural Language Processing (NLP), which is a subfield of artificial intelligence (AI) and linguistics that focuses on the interaction between computers and humans through natural language. The primary aim of NLP is to enable machines to understand, interpret, and generate human language in a way that is valuable.\n",
    "\n",
    "### Brief History:\n",
    "\n",
    "1. **1950s:** The dawn of NLP is often associated with Alan Turing’s question, “Can machines think?” He introduced the Turing test to determine a machine's ability to exhibit intelligent behavior indistinguishable from a human.\n",
    "  \n",
    "2. **1960s:** The first attempts at machine translation took place, driven by the Cold War needs. These were rule-based systems, attempting to directly translate words from one language to another. The ALPAC report in 1966 criticized the lack of progress and led to a significant reduction in funding for machine translation research.\n",
    "\n",
    "3. **1970s-1980s:** The focus shifted to rule-based approaches in various NLP tasks, including parsing. Noam Chomsky's linguistic theories, especially the idea of a universal grammar, were influential. This period also saw the birth of ELIZA, a primitive chatbot that could simulate conversation based on pattern matching.\n",
    "\n",
    "4. **1990s:** Statistical NLP emerged as an alternative to rule-based systems. This period saw increased use of probabilistic models and data-driven methods, with the Brown Corpus playing a crucial role in the development of statistical methods.\n",
    "\n",
    "5. **2000s:** The availability of vast amounts of digital data and powerful computational resources led to the rise of machine learning approaches in NLP. Algorithms like Latent Dirichlet Allocation (LDA) for topic modeling and Conditional Random Fields (CRFs) for sequence modeling became popular.\n",
    "\n",
    "6. **2010s:** Deep learning began to dominate NLP, with neural network architectures like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer architectures (e.g., BERT, GPT) achieving state-of-the-art results on various tasks. Transfer learning and pre-trained language models became the standard for many NLP applications.\n",
    "\n",
    "### Key Areas in NLP:\n",
    "\n",
    "1. **Machine Translation:** Automatically translating text from one language to another.\n",
    "  \n",
    "2. **Sentiment Analysis:** Determining the mood or sentiment expressed in a piece of text.\n",
    "  \n",
    "3. **Speech Recognition:** Translating spoken language into written form.\n",
    "  \n",
    "4. **Information Retrieval:** Finding relevant documents or data based on queries, e.g., search engines.\n",
    "  \n",
    "5. **Text Summarization:** Creating concise summaries of longer texts.\n",
    "  \n",
    "6. **Chatbots & Conversational Agents:** Simulating human-like conversation.\n",
    "\n",
    "7. **Named Entity Recognition:** Identifying and classifying named entities (e.g., names, locations) in text.\n",
    "\n",
    "8. **Topic Modeling:** Identifying topics present in a corpus.\n",
    "\n",
    "### Significance:\n",
    "\n",
    "NLP holds significance in various domains, from assisting in clinical diagnoses based on patient records, to powering recommendation systems in e-commerce, to facilitating real-time multilingual conversations.\n",
    "\n",
    "As with many areas in AI, challenges in NLP arise from the complexity and variability of human language. Homonyms, sarcasm, cultural context, and evolving language use can all confound simple computational methods. However, ongoing research in NLP continues to push the boundaries of what machines can understand and generate in terms of human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp reviews\n",
    "\n",
    "In this lesson we are going to explore visualizing the text of a random subset of 2000 Yelp business reviews.  Let's import and view the first few rows of the data. We're also going to filter the data to just include only sushi restaurant reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the text of the review looks exactly like what you would read from a review online.  It doesn't naturally lend itself to being classified as categorical or quantitative or viewed over space or time.  \n",
    "\n",
    "However, there are ways that we can learn about pattenrns in the text.  For example, it would be reasonable to think that different words are more likely to appear in really good (5-star) reviews than in really bad (low-star reviews).  Perhaps \"terrible\" appears much more in low-star reviews.  A 5-star restaurant might be \"terribly good\", but there are probably a lot more \"terribles\" in low-star than 5-star reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "\n",
    "A **word cloud**, also known as a tag cloud or text cloud, is a visual representation of text data. It displays a list of words, with the importance of each word being represented by its font size, color, or other visual distinctions. Typically, more frequently appearing words are shown in a larger font size and often in a more distinct color, while less frequent words are displayed in smaller fonts.\n",
    "\n",
    "Word clouds are useful for quickly perceiving the most prominent terms in a piece of text and can be a good starting point in text analysis to identify key themes or terms. They are often used to visualize the most frequent terms in datasets like surveys, feedback, reviews, speeches, and more.  You've probably seen them before.\n",
    "\n",
    "It would be difficult to make a word cloud on our own; however, Pandas comes to the rescue with the `wordcloud library`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes a nice visual, but of course there are limitations.  Words like \"food\" and \"place\" show up a lot but don't really give us insight into the sentiment of the reviewer.  Because each word is being taken independently without context a word like \"good\" shows up in the bad review word cloud, but it might have been part of a review that said \"not very good\".  We have no way to know.  On the other hand, the good review word cloud has a much bigger \"good\" and the bad review cloud has a much bigger \"wrong\" and \"spotty\".\n",
    "\n",
    "Concerns about using word clouds as a visualization include:\n",
    "\n",
    "- **Lack of Precision**: Word clouds give a general idea about data but don't provide precise frequency counts.\n",
    "  \n",
    "- **Context Loss**: Words are displayed outside of their original context, which can lead to misinterpretations.\n",
    "\n",
    "- **Overemphasis on Common Words**: Without proper preprocessing, common words (like \"and\", \"the\", \"is\") can dominate the visualization unless they are removed or filtered out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can improve our word clouds is by removing common words (also known as \"stop words\") from the text.  These are words that are frequent but don't really contain any information such as \"will\", \"go\", or \"and\".  We could also pick out context specific words like \"food\" to remove.  \n",
    "\n",
    "We're doing to save this kind of data engineering for later when you study natural language process in other classes.  Instead, let's look at another figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "\n",
    "Another way to look at the data is to see how different words factor into the overall sentiment of a review.  There are different ways to do this, but one way is by using the textblob library (Python has tons of NLP libraries) to compute the sentiment of each review.  Sentiment can be positive (a positive value), negative (a negative value) or neutral (a value at or near 0).  \n",
    "\n",
    "Interestingly, the sentiment calculated by textblob doesn't align perfectly (though it does generally match up) with the star ratings given by the users.  \n",
    "\n",
    "Below graph the distribution of sentiment for four words: \"\"disgusting\", \"delicious\", \"overpriced\", and \"delightful\".  You can think of the curved graphs in the visualization like the outline of the top of a histogram.  The highest point of the curved line indicates the most common sentiment for that term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sentiment for both \"delightful\" and \"delicious\" is almost totally positive.  \"Disgusting\" is negative, but not completely, and \"overpriced\" actually trends positive.  \n",
    "\n",
    "Take some time to experiment with the words plotted in the graphic and see what the overall sentiment of the reviews they come from is.  If a word isn't in the text of the reviews, it won't show up on the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review length\n",
    "\n",
    "What if simply the length of the review - without even examining any of the words - provides a clue if the review is positive or negative?  Do people leave long reviews when they are really disappointed in an experience?  Or do they love to share praise?  We can plot the mean number of characters (letters, numbers, punctuation), in the reviews that earned different numbers of stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the review lengths?  What star rating has the longest reviews, on average?  Do 1-star reviews tend to be longer or shorter than 5-star reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Visualizing Text Data\n",
    "\n",
    "1. **Simplicity is Key**: Given the high dimensionality, visual simplicity is often crucial for making insights accessible.\n",
    "\n",
    "2. **Dimensionality Reduction**: Techniques like t-SNE, PCA, or UMAP (you'll learn more about these later) can be used to reduce the dimensionality of text data for visualization.\n",
    "\n",
    "3. **Interactivity**: Text data often benefits from interactive visualizations that allow for exploration, such as clicking on a point in a scatter plot to see the associated text.\n",
    "\n",
    "4. **Context Matters**: Always provide enough context to interpret the visualization correctly. For example, if you're displaying frequently occurring terms, knowing the data source, and the text's general context is crucial for interpretation.\n",
    "\n",
    "5. **Multi-level Visualizations**: Sometimes, it helps to provide visualizations at different levels of granularity. For example, you could have one plot showing sentiment analysis at the document level and another at the sentence or paragraph level.\n",
    "\n",
    "6. **Use of Color and Size**: Carefully use color and size to indicate different attributes like frequency or sentiment. But avoid overcomplicating the visual by using too many visual cues simultaneously.\n",
    "\n",
    "7. **Comparative Visualizations**: Comparing text data across different categories, times, or conditions can often lead to more actionable insights.\n",
    "\n",
    "8. **Text Annotations**: Because text data is inherently rich and unstructured, adding text annotations to describe specific points or trends in your plot can be especially helpful.\n",
    "\n",
    "9. **Iterate and Refine**: Given the complexity of text data, you'll often need to iterate and refine your visualizations to ensure they're providing valuable insights.\n",
    "\n",
    "10. **User Testing**: If the visualization is meant for a broader audience, user testing can be beneficial. This helps ensure that the visualization is not only accurate but also intuitive and insightful for the end-user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK\n",
    "There are many amazing uses for natural language processing.  We were able to touch on visualizing only a very few of them, but hopefully this gave you a sense of the power of data visualizations even for unstructured data like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
